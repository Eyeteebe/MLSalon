{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNxDjESY/m9vg3d14xzh9Yo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eyeteebe/MLSalon/blob/master/resources/word2vec_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nqeis6aV-_6",
        "outputId": "3f12897a-f18a-4744-f13b-1a7d3422dab8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pprint\n",
        "import re\n",
        "from lxml import etree\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import FastText\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('ted_en-20160408.xml')\n",
        "\n",
        "targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "\n",
        "# Getting contents of <content> tag from the xml file\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# Removing \"Sound-effect labels\" using regular expression (i.e. (Audio), (Laughter))\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# Tokenising the sentence to process it by using NLTK library\n",
        "sent_text=sent_tokenize(content_text)\n",
        "\n",
        "# Removing punctuations and changing all characters to lower case\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# Tokenising each sentence to process individual word\n",
        "sentences=[]\n",
        "sentences=[word_tokenize(sentence) for sentence in normalized_text]\n",
        "\n",
        "# Prints only 10 (tokenised) sentences\n",
        "print(sentences[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation'], ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing'], ['consider', 'facit'], ['i', 'm', 'actually', 'old', 'enough', 'to', 'remember', 'them'], ['facit', 'was', 'a', 'fantastic', 'company'], ['they', 'were', 'born', 'deep', 'in', 'the', 'swedish', 'forest', 'and', 'they', 'made', 'the', 'best', 'mechanical', 'calculators', 'in', 'the', 'world'], ['everybody', 'used', 'them'], ['and', 'what', 'did', 'facit', 'do', 'when', 'the', 'electronic', 'calculator', 'came', 'along'], ['they', 'continued', 'doing', 'exactly', 'the', 'same']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBQb875tbKLL"
      },
      "source": [
        "def splite_words(formula):\n",
        "  word_list_n = []\n",
        "  word_list_p = []\n",
        "  single_word = \"\"\n",
        "  word_len = 0\n",
        "  word_group = 1\n",
        "  for bit in formula:\n",
        "    if bit == '-':\n",
        "      if word_group == 0:\n",
        "        word_list_n.append(single_word[:word_len])\n",
        "      else:\n",
        "        word_list_p.append(single_word[:word_len])\n",
        "      word_len = 0\n",
        "      single_word = \"\"\n",
        "      word_group = 0\n",
        "    elif bit == '+':\n",
        "      if word_group == 0:\n",
        "        word_list_n.append(single_word[:word_len])\n",
        "      else:\n",
        "        word_list_p.append(single_word[:word_len])\n",
        "      word_len = 0\n",
        "      single_word = \"\"\n",
        "      word_group = 1\n",
        "    else:\n",
        "      single_word += bit\n",
        "      word_len += 1\n",
        "\n",
        "  if word_group == 0:\n",
        "    word_list_n.append(single_word[:word_len])\n",
        "  else:\n",
        "    word_list_p.append(single_word[:word_len])\n",
        "\n",
        "  return word_list_n, word_list_p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGK3a9YdJ5-P"
      },
      "source": [
        "def get_4_models(sentences):\n",
        "  wv_cbow_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=5, workers=4, sg=0)\n",
        "  wv_sg_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=5, workers=4, sg=1)\n",
        "  ft_cbow_model = FastText(sentences, vector_size=100, window=5, min_count=5, workers=4, sg=0)\n",
        "  ft_sg_model = FastText(sentences, vector_size=100, window=5, min_count=5, workers=4, sg=1)\n",
        "  return wv_cbow_model, wv_sg_model, ft_cbow_model, ft_sg_model\n",
        "\n",
        "\n",
        "wv_cbow_model, wv_sg_model, ft_cbow_model, ft_sg_model = get_4_models(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doDY0tnxNH-s",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a0e5af-d759-4c7c-bad5-9230d28b6fe1"
      },
      "source": [
        "#@title Word Algebra Calculator\n",
        "\n",
        "#@markdown Please select the model and formula to calculate the word algebra\n",
        "\n",
        "# Get the input\n",
        "Formula = 'Family - parents' #@param {type: \"string\"}\n",
        "Formula = re.sub(\" \", \"\", Formula.lower())\n",
        "word_list_p, word_list_n = splite_words(Formula)\n",
        "#print(word_list_n, word_list_p)\n",
        "Model = 'Word2Vec' #@param ['Word2Vec', 'FastText']\n",
        "Architecture = \"Skip Gram\" #@param [\"CBOW\", \"Skip Gram\"]\n",
        "if Model == \"Word2Vec\":\n",
        "  if Architecture == \"CBOW\":\n",
        "    result = wv_cbow_model.wv.most_similar(positive=word_list_p, negative=word_list_n, topn=1)\n",
        "  else:\n",
        "    result = wv_sg_model.wv.most_similar(positive=word_list_p, negative=word_list_n, topn=1)\n",
        "else:\n",
        "  if Architecture == \"CBOW\":\n",
        "    result = ft_cbow_model.wv.most_similar(positive=word_list_p, negative=word_list_n, topn=1)\n",
        "  else:\n",
        "    result = ft_sg_model.wv.most_similar(positive=word_list_p, negative=word_list_n, topn=1)\n",
        "print(\"The result is:\", result[0][0])\n",
        "\n",
        "#@markdown Now you can activate the Calculator by running this section"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The result is: kids\n"
          ]
        }
      ]
    }
  ]
}